---
title: "Distributed training with DistributedDataParallel" 
excerpt: "Ultilize multiple GPUs accross multiple machines for your training with a simple model wrapper."
header:
  teaser: "assets/images/torch.png"
tags: 
  - pytorch
  - code
---

This is the final part of a 3-part series covering multiprocessing, distributed communication, and distributed training in PyTorch.

In this article, let's first see how we can use `torch.nn.parallel.DistributedDataParallel` for distributed training, and then dive under the hood to see some algorithmic details of how DDP works.

## DDP quick start

### Data parallelism and model parallelism

### A local training example
We start with this piece of local training code on MNIST:

Import necessary libraries:
```python
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
```

Define an arbitrary model:
```py
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1)
        self.n1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)
        self.n2 = nn.BatchNorm2d(64)
        self.fc1 = nn.Linear(1600, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.n1(x)
        x = F.relu_(x)
        x = F.max_pool2d(x, 2)

        x = self.conv2(x)
        x = self.n2(x)
        x = F.relu_(x)
        x = F.max_pool2d(x, 2)

        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu_(x)
        x = self.fc2(x)
        x = F.log_softmax(x, dim=-1)

        return x
```

We can use 2 utility functions:
```py
def get_mnist_loader(train=True, **kwargs):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    dataset = datasets.MNIST('./data', train=train, download=True, transform=transform)
    loader = torch.utils.data.DataLoader(dataset, **kwargs)
    return loader

def train(model, optimizer, train_loader, device):
    for X, target in train_loader:
        X = X.to(device)
        target = target.to(device)
        optimizer.zero_grad()
        pred = model(X)
        loss = F.nll_loss(pred, target)
        loss.backward()
        optimizer.step()
```

The good thing is, later on, we won't touch the above code.

Finally, define our main function:
```py
if __name__ == "__main__":
    cuda = True
    device = torch.device('cuda' if cuda else 'cpu')
    
    model = Net().to(device)
    train_loader = get_mnist_loader(train=True, batch_size=64, shuffle=True)
    optimizer = torch.optim.Adadelta(model.parameters(), lr=0.01)

    model.train()
    for _ in range(5):
        train(model, optimizer, train_loader, device)
```

Nothing out of the ordinary here. Most local training code follows the same routine as this example. Now let's add distributed training to this code.

### Distributed training with DDP
To use DDP, we first have to initialize the process group and the `torch.distributed` package. In-depth discussion of the initialization process as well as the package `torch.distrbuted` itself can be found in the previous [article](/distributed-communication-in-pytorch).

Most commonly, we initialize the process group via TCP using the environment variable method. We would need the master node's address and port number—to set up all communications—, as well as the world size and rank of the current process. We use NCCL backend for GPU training and Gloo backend for CPU training:

```python
import torch.distributed as dist
...
if __name__ == "__main__":
    cuda = True
    # Init torch.distributed
    dist.init_process_group('nccl' if cuda else 'gloo')
    device = torch.device('cuda' if cuda else 'cpu')
    ...
```

The init call is equivalent to `dist.init_process_group(..., init_method='env://')`. This also means that all the information needed to initialize the process group should be provided by the environment variables, but how? We will handle this at launch time using `torchrun`, but let's not worry about that for now.

We then simply wrap our model with `DistributedDataParallel`. Let's start with the easy case where we run one process per node. For example, this is used when we have **multiple nodes, one GPU per node**. Since we don't have to worry about using the correct device (GPU) for each process, the wrapping process is as simple as:

```python
from torch.nn.parallel import DistributedDataParallel as DDP
...
if __name__ == "__main__":
    cuda = True
    # Init torch.distributed
    dist.init_process_group('nccl' if cuda else 'gloo')
    device = torch.device('cuda' if cuda else 'cpu')
    # Define and wrap model with DDP
    model = Net().to(device)
    model = DDP(model)
    ...
```

And voilà, we are ready to go! See [`torchrun`](#torchrun) for details on how to launch the training process.

### Multiple GPUs per node
Now, let's consider the case where we want to run multiple processes per node. In practical uses, this often corresponds to the **multiple nodes, multiple GPUs per node** scenario. In this case, each process within a node must be assigned to a specific GPU.
Assuming that we know the number of GPUs $$N$$ of node A, we will launch $$N$$ processes on node A. Each process launched by `torchrun` will automatically be given a set of environment variables, including `WORLD_SIZE`, `RANK`, and `LOCAL_RANK`. `LOCAL_RANK` denotes the rank of a process **within its node**, while `RANK` denotes the rank of a process **across all nodes**, or globally.

So, launching $$N$$ processes within a node will give the processes `LOCAL_RANK`s from 0 to $$N-1$$. We can use them as indices to access corresponding GPUs.

```python
...
if __name__ == "__main__":
    cuda = True
    # Init torch.distributed
    dist.init_process_group('nccl' if cuda else 'gloo')
    # Get LOCAL_RANK
    LOCAL_RANK = int(os.getenv('LOCAL_RANK', 0))
    # Set device
    if cuda:
        torch.cuda.set_device(LOCAL_RANK)
        device = torch.device(f'cuda:{LOCAL_RANK}')
    else:
        device = torch.device('cpu')
    # Define and wrap model with DDP
    model = Net().to(device)
    if cuda:
        model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)
    else:
        model = DDP(model)
    ...
```

### torchrun

## Under the hood

## Closing remarks